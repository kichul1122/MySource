# Memory

# 메모리의 종류
1.메인메모리 : 램(RAM) D램
2.레지스터(Register) : CPU안에 내장 되어 있어서 연산을 위한 저장소 제공
3.캐쉬(Cache) : S램. CPU와 램 사이에서 중간 저장소 역할
4.하드디스크(Hard Disk)와 이외 저장 장치 : 하드디스크, I/O 장치 등등

![메모리 계층](https://github.com/kichul1122/MySource/blob/master/Assets/Memory/Memory%20Hierarchy.jpg)


메모리들은 프로그램이 실행하는 동안 데이터의 입력 및 출력을 담당한다.

메모리들의 차이는 CPU 와의 거리에서 온다.

CPU와의 거리가 가까울수록 빠르고 용량이 작으며 멀수록 느리고 용량이 크다.(기술과 돈의 문제)

하드디스크에 있는 내용은 프로그램의 실행을 위해 메인 메모리로 이동한다.

메인 메모리에 있는 일부 데이터도 실행을 위해 L2 캐시로 이동한다.

L2 캐시에 있는 데이터 일부는 L1 캐시로 이동한다.

L1 캐시에 있는 데이터중 연산에 필요한 데이터는 레지스터로 이동한다.


반대로 연산에 필요한 데이터가 레지스터에 없으면 L1 캐시를 살펴본다. 없으면 L2캐시 없으면 메인 메모리,

그래도 없으면 하드디스크를 참조한다. 하드디스크에서 데이터를 찾은 후 다시 메인 메모리 L2 캐쉬 L1 캐시를 거쳐 

레지스터로 데이터가 들어오게 되는데 이경우 극심한 속도저하가 발생한다.

![메모리 관계](https://github.com/kichul1122/MySource/blob/master/Assets/Memory/Memory%20Hierarchy.jpg)

(참고 : 

캐시를 없애 중간단계를 줄이는 것이 속도가 빠르지 않냐 생각할수 있는데
L1 캐시와 L2 캐시에, 연산에 필요한 데이터가 존재할 확률이 90% 이상이다.따라서 캐시는 속도향상에 도움을 준다)



L1 캐시와 L2 캐시 :

시스템의 성능을 좌우하는 클럭속도는 느린쪽에 맞춰진다.

CPU는 고속화되었지만 메인 메모리의 처리속도는 이를 따라가지 못한다.

CPU가 연산을 하기 위해선 데이터를 가지고 와서 연산을 한 후 연산결과를 메모리에 저장한 후에

다음작업을 수행할 수 있다.

따라서 아무리 CPU가 빠르게 연산을 수행한다 하더라도 데이터를 가지오고 저장하는 작업이 느리다면

전체적인 처리속도는 결코 빠를수 없다.

L1캐시는 이러한 레지스터와 메인 메모리간의 속도차이에 의한 성능저하를 막기 위해

메인 메모리의 저장된 데이터 중 자주 접근하는 데이터를 저장한다.

L1 캐시는 CPU 내부에 존재하므로 L1 캐시에서 데이터를 참조할 경우 속도저하는 발생하지 않는다.

하지만 여전히 L1 캐시는 메인 메모리의 모든 데이터를 저장할 수 없기에 L1 캐시에 없는 데이터를 

CPU가 요구할 경우 속도의 저하로 이어진다.

따라서 캐시를 하나 더둔다.(L1 캐시에 용량을 증가시키는데ㄷ에도 한계가 있다(돈과 기술))

L2 캐시까지 존재함으로써 메인 메모리에 대한 접근은 더욱 줄어든다.

따라서 병목현상은 L1캐시와 메인 메모리에서 L2 캐시와 메인 메모리로 발생지역이 옮겨지게 된다.



캐쉬(Cache)와 캐쉬 알고리즘 :

템퍼럴 로컬리티(Temporal Locality) : 한번 접근이 이뤄진 주소의 메모리 영역은 자주 접근한다.

스페이셜 로컬리티(Spatial Locality) : 접근하는 메모리 영역은 이미 접근이 이루어진 영역의 근처일 확률이 높다.

캐시 프렌드리 코드(Cache Friendly Code) : 템퍼럴 로컬리티와 스페이셜 로컬리티를 최대한 활용하여 캐시의 도움을 받을수 있도록 구현한 코드



캐시 알고리즘 :


캐시 힛(Cache Hit) : 연산에 필요한 데이터가 L1 캐시에 존재할 경우


캐시 미스(Cache Miss) : 연산에 필요한 데이터가 L1 캐시에 존재 하지 않을 경우
(참고 : 이경우 L2 캐시를 검사하며 L2 캐시 미스가 발생하면 메인 메모리에서 데이터를 가져온다)


데이터의 이동은 블록 단위로 진행하여 스페이셜 로컬리티의 특성을 성능향상에 활용한다.
(예 : 0x10000 번지의 데이터를 요청하면 0x10000을 포함한 블록 전체가 전송된다)


(참고 : 현재 L2 캐시는 CPU 내부에 존재한다)

메모리 계층 아래로 갈수록 전송되는 블록 크기가 커진다.

아래에 존재하는 메모리에 대한 접근 횟수를 줄여준다.


캐시 교체 정책(Cache's Replacement Policy) : 

프로그램이 실행된느 동안 모든 메모리는 항상 채워져 있다.

메모리가 꽉 채워져 있어요 요구하는 데이터를 가지고 있을 확률이 높아지기 때문이다.

이때문에 가지고 있지 않은 데이터를 요구할 경우 메모리가 꽉 찾기 때문에 메모리 블록을 교체해야 한다.

블록 교체 알고리즘은 캐시 교체 정책에 의해 달라진다.
(참고 : 

대표적 블록 교체 알고리즘 : 
LRU(Least-Recently Used) : 가장 오래 전에 참조된 블록을 밀어내는 알고리즘)

출처: http://dakuo.tistory.com/126 [hacker dakuo]